{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Guía 7 - Validación Cruzada y Optimización de Hiperparámetros.\n",
    "\n",
    "Un/a Cientifico/a de Datos suele - en general - entrenar modelos de Aprendizaje Asutomático para obtener predicciones sobre nuevos datos y, en el medio, aprender sobre el problema que le interesa. Un buen modelo no es aquel que da predicciones exactas sobre los datos conocidos (o datos de entrenamiento), sino el que devuelve buenas predicciones sobre nuevos datos (datos desconocidos o nunca vistos), evitando tanto como sea posible los problemas de sobreajuste (overfitting) y de subajuste (underfitting). Recordemos que:\n",
    "\n",
    "* El sobreajuste se produce cuando un modelo de aprendizaje automático captura el ruido de los datos. Intuitivamente, el sobreajuste se produce cuando el modelo se ajusta demasiado bien a los datos o, como a veces se dice coloquialmente, “el modelo aprende de memoria los datos”. Este ajuste excesivo da como resultado un buen desempeño en el conjunto de datos de entrenamiento, pero resultados pobres en datos nunca vistos. \n",
    "\n",
    "* Por el contrario, el subajuste se produce cuando un modelo no puede capturar la tendencia subyacente de los datos. Intuitivamente, la inadecuación se produce cuando el modelo no generaliza lo suficientemente bien a los datos. Este subjuaste suele ser el resultado de un modelo excesivamente simple.\n",
    "\n",
    "Ambas situaciones se pueden presentar en problemas de clasificación como de regresión. Hemos visto que la primera forma de evaluar cuán bien generaliza un modelo es utilizándolo sobre datos no *vistos* durante el entrenamiento, es decir, haciendo un train-test split. Cuando optimizamos hiperparámetros, además, vimos que solemos utilizar tres conjuntos, entrenamiento-validación-evaluación. No olvidemos, también, que una forma de evitar el sobreajuste es introduciendo regularización. Ahora, consideremos las siguientes situaciones:\n",
    "\n",
    "1. Contamos con pocos datos. Hacer un train-test split por un lado deja datos en *test* que son valiosos para el entrenamiento. Por otro lado, dejar pocos datos en *test* hace que la evaluación en ese conjunto sea poco robusta.\n",
    "1. Queremos estudiar si incorporar un nuevo atributo a nuestro modelo redunda en mejor poder predictivo para nuestro problema de regresión. Hacemos un train-test split, entrenamos sin y con el atributo y calculamos el RMSE en test. Para el modelo sin el nuevo atributo, nos da 0.86; con el nuevo atributo, 0.83. ¡Bárbaro! Parece ser que incorporar el nuevo atributo mejora el desempeño. Volvemos a ejecutar nuestro código, y ahora notamos que los nuevos RMSE son 0.84 y 0.85, respectivamente. ¿Qué ocurrió?\n",
    "1. Tenemos un modelo y queremos optimizar algunos de sus hiperparámetros. En total, son cuatro hiperparámetros, cada uno con 5 o 6 posibles valores. En total, debemos entrenar y evaluar $5 x 6^3  = 1080$ modelos. \n",
    "\n",
    "Veamos esta situación más en detalle. En algunos problemas financieros - por ejemplo, predecir si una acción va a subir o bajar en los próximos días - los desempeños suelen estar muy cerca del 0.5 para datasets balanceados (refiriéndonos a exactitud en este caso). Desempeños del 0.6 ya son considerados muy buenos, por no decir inalcanzables en muchísimas situaciones.\n",
    "\n",
    "A continuación, vamos a cargar un dataset que contiene 1200 instancias, 5 atributos y una variable a predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IAA_Guia_8_data.csv')\n",
    "X = df.drop('y', axis=1)\n",
    "y = df.y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Separar en conjuntos de entrenamiento, validación y evaluación. Cada uno con 850, 150 y 250 instancias, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = COMPLETAR(COMPLETAR, random_state = 2023)\n",
    "X_train, X_val, y_train, y_val = COMPLETAR(X_train, y_train, COMPLETAR, random_state = 2023)\n",
    "                                           \n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**: optimizar hiperparámetros para un Árbol de decisión usando entranamiento (*train*) y validación (*val*). Evaluar el desempeño del mejor modelo en evaluación (*test*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos los hiperparámetros a optimizar y sus posibles valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_list = [1,3,5,7,9,11]\n",
    "min_samples_split_list = [10,20,30,40,50,60]\n",
    "min_samples_leaf_list = [10,20,30,40,50,60]\n",
    "max_features_list = [1,2,3,4,5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y entrenamos sobre todas las combinaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2023)\n",
    "df_results = pd.DataFrame()\n",
    "counter = 0\n",
    "\n",
    "### Recorremos todas las combinaciones. Un loop por cada una.\n",
    "for max_depth in max_depth_list:\n",
    "    for COMPLETAR:\n",
    "        for COMPLETAR:\n",
    "            for COMPLETAR:\n",
    "                tree = DecisionTreeClassifier(COMPLETAR)\n",
    "                tree.COMPLETAR\n",
    "\n",
    "                # Predecimos sobre nuestro set de entrenamieto\n",
    "                y_train_pred = COMPLETAR\n",
    "\n",
    "                # Predecimos sobre nuestro set de validacion\n",
    "                y_val_pred = COMPLETAR\n",
    "                \n",
    "                # Guardamos resultados\n",
    "                df_results.loc[counter, 'max_depth'] = max_depth\n",
    "                df_results.loc[counter, 'min_samples_split'] = min_samples_split\n",
    "                df_results.loc[counter, 'min_samples_leaf'] = min_samples_leaf\n",
    "                df_results.loc[counter, 'max_features'] = max_features\n",
    "                df_results.loc[counter, 'accuracy_train'] = COMPLETAR\n",
    "                df_results.loc[counter, 'accuracy_val'] = COMPLETAR\n",
    "                counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cuál es el mejor modelo para nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.iloc[df_results.accuracy_val.argmax()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debería dar algo así como:\n",
    "\n",
    "```\n",
    "max_depth            11.000000\n",
    "min_samples_split    30.000000\n",
    "min_samples_leaf     30.000000\n",
    "max_features          4.000000\n",
    "accuracy_train        0.640000\n",
    "accuracy_val          0.593333\n",
    "```\n",
    "\n",
    "\n",
    "Si te dio así, ¡un éxito! El modelo no parece estar sobreajustado. Además, el desempeño está bastante por encima del desempeño que asumiamos a priori. Si fuera un problema financiero de verdad, estás listo para hacerte rico.\n",
    "\n",
    "Veamos su desempeño en `test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = COMPLETAR\n",
    "COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probablemente te llevaste una desilusión. ¿Qué ocurrió?\n",
    "\n",
    "Fíjense que nuestro conjunto de validación tiene 150 instancias, y nosotros probamos 1080 modelos. La probabilidad de que alguno de ellos pareciera andar bien por mera casualidad es altísima. ¿Cómo podríamos hacer para evitarnos este problema? La respuesta a esta pregunta - y a todas las situaciones planteadas previamente - es bastante intuitiva si la piensas, Validación Cruzada. \n",
    "\n",
    "**Ejercicio:** introducir validación cruzada en el código anterior. Eso implica agregar un for-loop más. Si bien nos va a quedar un código bastante cargado, por ahora es bien explícito. Luego veremos una forma mucho más compacta de hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VOLVEMOS A DEFINIR X_TRAIN Y X_TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=250, \n",
    "                                                    random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2023)\n",
    "df_results = pd.DataFrame()\n",
    "N_folds = 10\n",
    "counter = 0\n",
    "\n",
    "### Recorremos todas las combinaciones. Un loop por cada una.\n",
    "for max_depth in max_depth_list:\n",
    "    for COMPLETAR:\n",
    "        for COMPLETAR:\n",
    "            for COMPLETAR:\n",
    "                tree = DecisionTreeClassifier(COMPLETAR)\n",
    "                \n",
    "                # Guardamos resultados\n",
    "                df_results.loc[counter, 'max_depth'] = max_depth\n",
    "                df_results.loc[counter, 'min_samples_split'] = min_samples_split\n",
    "                df_results.loc[counter, 'min_samples_leaf'] = min_samples_leaf\n",
    "                df_results.loc[counter, 'max_features'] = max_features\n",
    "                \n",
    "                ### AGREGAMOS ACÁ LA CV\n",
    "                \n",
    "                for fold in range(N_folds):\n",
    "                    ### NOTAR QUE NO ES EXACTAMENTE K-FOLD CV. ¿POR QUÉ?\n",
    "                    X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(COMPLETAR)\n",
    "                    \n",
    "                    tree.fit(X_train_fold, y_train_fold)\n",
    "                    \n",
    "                    y_train_fold_pred = COMPLETAR\n",
    "                    y_val_fold_pred = COMPLETAR\n",
    "\n",
    "                    df_results.loc[counter, f'accuracy_train_{fold}'] = COMPLETAR\n",
    "                    df_results.loc[counter, f'accuracy_val_{fold}'] = COMPLETAR\n",
    "                    \n",
    "                counter +=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos todas las exactitudes obtenidas por cada fold y cada entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[[f'accuracy_val_{fold}' for fold in range(N_folds)]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, podemos ver todas las exactitudes promedio obtenidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[[f'accuracy_val_{fold}' for fold in range(N_folds)]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[[f'accuracy_val_{fold}' for fold in range(N_folds)]].mean(axis=1).hist(rwidth=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y sus desviaciones estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[[f'accuracy_val_{fold}' for fold in range(N_folds)]].std(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver claramente como todas las exactitudes tienen una distribución alrededor del 0.5.\n",
    "\n",
    "## 2. Validación Cruzada con Scikit-Learn\n",
    "\n",
    "Veamos cómo hacer Validación Cruzada pero utilizando las funcionalidad de Scikit-Learn. Hay más de una forma de hacerlo. Recomendamos mirar la muy buena documentación sobre [validación cruzada de Scikit-Learn](https://scikit-learn.org/stable/modules/cross_validation.html). \n",
    "\n",
    "**Ejercicio**: utilizar la función `cross_val_score` para evaluar un `DecisionTreeClassifier` con hiperparámetros arbitrarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = COMPLETAR\n",
    "cross_val_score(COMPLETAR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**: reemplazar en el bloque de código donde optimizamos hiperparámetros y hacíamos validación cruzada la sección que corresponda por una evaluación con `cross_val_score` para cada modelo. Solamente guardar la exactitud promedio y su desviación estándar. Volver a hacer un histograma de las exactitudes promedio obtenidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2023)\n",
    "df_results = pd.DataFrame()\n",
    "N_folds = 10\n",
    "counter = 0\n",
    "\n",
    "COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.mean_accuracy.hist(rwidth=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimización de hiperparámetros\n",
    "\n",
    "**¿Podemos hacer directamente la exploración de hiperparámetros?** Sí, podemos. En el código, significaría deshacernos directamente de todos los for-loops.\n",
    "\n",
    "Para optimizar hiperparámetros necesitamos:\n",
    "* Una métrica, por ejemplo: exactitud, precisión, RMSE, ROC AUC, etc.\n",
    "* Un modelo: regresor o clasificador.\n",
    "* Un espacio de hiperparámetros. Depende del tipo de modelo que estemos usando.\n",
    "* Un método para buscar o muestrear los candidatos:\n",
    "    * Manual: basado en la experiencia, tedioso y poco eficiente, pero útil para obtener una primera intuición.\n",
    "    * Grid Search: Plantea opciones y explora todas las combinaciones\n",
    "    * Random Search: explora opciones y combinaciones al azar.\n",
    "\n",
    "\n",
    "**Ejercicio:** lee acerca de [`GridSearchCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) y [`RandomizedSearchCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html).\n",
    "\n",
    "**Ejercicio:** aplica la función `GridSearchCV` de Scikit-Learn al problema en el que venimos trabajando. Analiza los resultados. Responde: ¿Cómo elijo la mejor configuración? ¿Cuál es la mejor performance? ¿Y el resto de los resultados?\n",
    "\n",
    "**Pistas:** La respuesta correcta siempre se encuentra en la documentación. `best_params_`, `best_score_` y `cv_results_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Por las dudas volvemos a hacer la separación\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=250, \n",
    "                                                    random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grilla para Grid Search\n",
    "max_depth_list = [1,3,5,7,9,11]\n",
    "min_samples_split_list = [10,20,30,40,50,60]\n",
    "min_samples_leaf_list = [10,20,30,40,50,60]\n",
    "max_features_list = [1,2,3,4,5]\n",
    "\n",
    "param_grid = {COMPLETAR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mejores parametros: \"+str(model.COMPLETAR))\n",
    "print(\"Mejor Score: \"+ str(model.COMPLETAR)+'\\n')\n",
    "\n",
    "scores = pd.DataFrame(model.COMPLETAR)\n",
    "scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** explorar la función `RandomizedSearchCV` de Scikit-Learn y utilízala.\n",
    "\n",
    "**Para pensar**: si tuvieras que arriesgar, ¿creés que hay señal en el dataset utilizado?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Ejercitación\n",
    "\n",
    "Aplica lo visto en alguno de los siguientes datasets: el de bicicletas que utilizamos al principio de la cursada (predicción de cantidad de viajes por día) o utiliza el dataset de cáncer de mama Wisconsin, disponible en Scikit-Learn. Optimiza hiperparámetros de al menos un modelo (dos preferiblemente). Si lo haces sobre dos modelos, ¿Cuál eligirías?¿La evaluación sobre qué conjunto te ayudará a hacer esa elección?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.c_[data['data'], data['target']],\n",
    "                  columns= np.append(data['feature_names'], ['target']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulario de Asistencia\n",
    "\n",
    "Obligatorio completar antes del Miercoles 24 de Mayo a las 23:59\n",
    "https://forms.gle/h6iL3RSkfQhRS7fW6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
